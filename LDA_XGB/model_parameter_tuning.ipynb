{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69433ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX\n",
      "AD        72\n",
      "svPPA     59\n",
      "bvFTD     53\n",
      "nfvPPA    46\n",
      "DLB       25\n",
      "PD        24\n",
      "SVAD      24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_processor import *\n",
    "from lda_model import LDATopicModel\n",
    "from classifier import TopicClassifier\n",
    "from visualizer import *\n",
    "from brain_visualizer import *\n",
    "\n",
    "data_path = 'C:/Users/BREIN/Desktop/stage_copath/data'\n",
    "# inp_df = pd.read_csv(data_path+'/260120_wsev_smc_combined_zscores.csv')\n",
    "inp_df = pd.read_csv(os.path.join(data_path, '260120_wsev_smc_combined_zscores.csv'))\n",
    "\n",
    "# inp_df = inp_df[~inp_df[\"DX\"].isin(['svPPA', 'nfvPPA'])] #######################\n",
    "print(inp_df['DX'].value_counts())\n",
    "\n",
    "\n",
    "df_nacc_resilience = pd.read_csv(data_path+'/nacc/NACC_resilience_inference.csv')\n",
    "df_adni4_resilience = pd.read_csv(data_path+'/adni/ADNI4_resilience_inference.csv')\n",
    "df_adni4_resilience = df_adni4_resilience.rename(columns={\"FULL_ID\": \"SUBJ_ID\"})\n",
    "df_nacc_resilience = df_nacc_resilience.rename(columns={\"subject_id\" : \"SUBJ_ID\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d54064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "\n",
      "===== Fold 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:57:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:57:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 3 =====\n",
      "\n",
      "===== Fold 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:57:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:57:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:57:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:57:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.5479\n"
     ]
    }
   ],
   "source": [
    "# LDA, CLASS-WEIGHTED XGBoost - RETIRED\n",
    "N_TOPICS = 18 ###\n",
    "region_cols = list(inp_df.loc[:, \"VA/2\":\"VA/2035\"].columns)\n",
    "labels = inp_df[\"DX\"].values\n",
    "ids = inp_df[\"SUBJ_ID\"].values\n",
    "\n",
    "# Fit LDA on combined z-scores\n",
    "lda = LDATopicModel(n_topics=N_TOPICS)\n",
    "theta = lda.fit_transform(inp_df[region_cols])\n",
    "\n",
    "# Fit classifier\n",
    "classifier = TopicClassifier(n_splits=5, class_weight='balanced') ##\n",
    "cv_results = classifier.cross_validate(theta, labels, ids, verbose=True)\n",
    "classifier.fit(theta, labels)\n",
    "\n",
    "print(f\"CV Accuracy: {cv_results['accuracy']:.4f}\")\n",
    "\n",
    "# visualizer = CopathologyVisualizer(\n",
    "#     output_dir=f'./model_parameter_tuning_results/topics_{N_TOPICS}_class_weighted'\n",
    "# )\n",
    "\n",
    "# fig_conf_mat = visualizer.plot_confusion_matrix(\n",
    "#     cm=classifier.get_confusion_matrix(),\n",
    "#     class_names=classifier._classes\n",
    "# )\n",
    "\n",
    "# fig_top_regions = visualizer.plot_top_regions_per_topic(\n",
    "#     topic_patterns = lda.get_topic_patterns(),\n",
    "#     region_names=region_cols\n",
    "# )\n",
    "\n",
    "# fig3 = visualizer.plot_diagnosis_topic_profiles(\n",
    "#     theta=lda._theta,\n",
    "#     dx_labels = labels\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e8eadc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX\n",
      "AD        25\n",
      "DLB       25\n",
      "bvFTD     25\n",
      "nfvPPA    25\n",
      "svPPA     25\n",
      "PD        24\n",
      "SVAD      24\n",
      "Name: count, dtype: int64\n",
      "k_topics =  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:14:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:14:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:14:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_topics 20, CV Accuracy: 0.4509\n",
      "Object `Copatholo` not found.\n"
     ]
    }
   ],
   "source": [
    "# Class Balanced - Downsample to 25\n",
    "## DOWNSAMPLE LARGE DX \n",
    "N = 25\n",
    "dx_col = \"DX\"\n",
    "balanced_parts = []\n",
    "\n",
    "for dx, g in inp_df.groupby(dx_col):\n",
    "    # if dx == 'AD':\n",
    "    #     N=50\n",
    "    # else: \n",
    "    #     N=25\n",
    "    if len(g) > N:\n",
    "        g = g.sample(n=N, replace=False, random_state=42)\n",
    "    balanced_parts.append(g)\n",
    "\n",
    "balanced_df = pd.concat(balanced_parts).reset_index(drop=True)\n",
    "print(balanced_df[dx_col].value_counts())\n",
    "####\n",
    "\n",
    "# for n in list(range(6, 25, 2)):\n",
    "n=20\n",
    "print('k_topics = ', n)\n",
    "N_TOPICS = n ###\n",
    "region_cols = list(balanced_df.loc[:, \"VA/2\":\"VA/2035\"].columns)\n",
    "labels = balanced_df[\"DX\"].values\n",
    "ids = balanced_df[\"SUBJ_ID\"].values\n",
    "\n",
    "# Fit LDA on combined z-scores\n",
    "lda = LDATopicModel(n_topics=N_TOPICS)\n",
    "theta = lda.fit_transform(balanced_df[region_cols])\n",
    "\n",
    "# Fit classifier\n",
    "classifier = TopicClassifier(n_splits=5) ##\n",
    "cv_results = classifier.cross_validate(theta, labels, ids, verbose=False)\n",
    "classifier.fit(theta, labels)\n",
    "\n",
    "print(f\"K_topics {n}, CV Accuracy: {cv_results['accuracy']:.4f}\")\n",
    "\n",
    "# visualizer = CopathologyVisualizer(\n",
    "#     output_dir=f'./model_parameter_tuning_results/topics_{N_TOPICS}_downsampled'\n",
    "# )\n",
    "\n",
    "# fig_conf_mat = visualizer.plot_confusion_matrix(\n",
    "#     cm=classifier.get_confusion_matrix(),\n",
    "#     class_names=classifier._classes\n",
    "# )\n",
    "\n",
    "# fig_top_regions = visualizer.plot_top_regions_per_topic(\n",
    "#     topic_patterns = lda.get_topic_patterns(),\n",
    "#     region_names=region_cols\n",
    "# )\n",
    "\n",
    "# fig3 = visualizer.plot_diagnosis_topic_profiles(\n",
    "#     theta=lda._theta,\n",
    "#     dx_labels = labels\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda._theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ca7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Surface Mapping \n",
    "## Topicwise Surface Maps\n",
    "from atlas_vis import DKTAtlas62ROIPlotter\n",
    "plotter_62  = DKTAtlas62ROIPlotter(\n",
    "    cmap='Reds',\n",
    "    clim=(0, 0.1),  \n",
    "    window_size=(1200, 1000),\n",
    "    nan_color='lightgray',\n",
    "    background='white',\n",
    "    template_key='pial'\n",
    ")\n",
    "os.makedirs(f'./model_parameter_tuning_results/topics_{n}_downsampled/topicwise',exist_ok=True)\n",
    "\n",
    "# df = pd.read_csv('./combined_results//lda_topic_atrophy_patterns.csv')\n",
    "# df = lda.get_topic_patterns().T\n",
    "topic_df = pd.DataFrame(\n",
    "    lda.get_topic_patterns().T,\n",
    "    index=region_cols,\n",
    "    columns=[f\"Topic_{k}\" for k in range(n)]\n",
    ")\n",
    "\n",
    "df = topic_df.tail(62).reset_index(drop=True)\n",
    "\n",
    "print(len(df))\n",
    "for col in df.columns: ##################\n",
    "    print(col)\n",
    "    l_values = df.loc[:30,col].to_list()\n",
    "    r_values = df.loc[31:,col].to_list()\n",
    "    print(len(l_values))\n",
    "    print(len(r_values))\n",
    "    print(np.min(l_values+r_values))\n",
    "    print(np.max(l_values+r_values))\n",
    "\n",
    "    plotter_62(l_values, r_values, save_path=f'./model_parameter_tuning_results/topics_{n}_downsampled/topicwise/{col}.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f345f",
   "metadata": {},
   "source": [
    "**NACC INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b5c58e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/BREIN/Desktop/copathology_visualization_temp/data/nacc/260120_NACC_VA_TAU_PATH_matched.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## NACC Inference ## 260120\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_processor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 3\u001b[0m nacc_raw \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/BREIN/Desktop/copathology_visualization_temp/data/nacc/260120_NACC_VA_TAU_PATH_matched.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# nacc_raw.rename(columns=col_map)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m region_cols \u001b[38;5;241m=\u001b[39m nacc_raw\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVA/2\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVA/2035\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[1;32mc:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\BREIN\\anaconda3\\envs\\data_management\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/BREIN/Desktop/copathology_visualization_temp/data/nacc/260120_NACC_VA_TAU_PATH_matched.csv'"
     ]
    }
   ],
   "source": [
    "## NACC Inference ## 260120\n",
    "from data_processor import *\n",
    "nacc_raw = pd.read_csv('C:/Users/BREIN/Desktop/copathology_visualization_temp/data/nacc/260120_NACC_VA_TAU_PATH_matched.csv')\n",
    "# nacc_raw.rename(columns=col_map)\n",
    "region_cols = nacc_raw.loc[:, 'VA/2':'VA/2035'].columns\n",
    "pathology_cols = nacc_raw.loc[:, 'NACC_AD':'NACC_svPPA'].columns\n",
    "nacc_filtered = nacc_raw[nacc_raw['DX'] != 'Unknown']\n",
    "\n",
    "nacc_cn = nacc_filtered[nacc_filtered['DX'] == 'CN']\n",
    "nacc_pat = nacc_filtered[nacc_filtered['DX'] != 'CN']\n",
    "# nacc_pat = nacc_filtered\n",
    "\n",
    "print(nacc_cn.shape)\n",
    "print(nacc_pat.shape)\n",
    "\n",
    "nacc_prep = DataProcessor(\n",
    "    region_cols=region_cols,\n",
    "    dx_col='DX',\n",
    "    subject_col='subject_id'\n",
    ")\n",
    "nacc_prep.fit_baseline(hc_data=nacc_cn)\n",
    "nacc_Z = nacc_prep.compute_atrophy_scores(data=nacc_pat)\n",
    "print(type(nacc_Z))\n",
    "\n",
    "nacc_theta = lda.transform(nacc_Z)\n",
    "y_pred = classifier.predict(nacc_theta)\n",
    "y_proba = classifier.predict_proba(nacc_theta)\n",
    "# print(nacc_theta.shape)\n",
    "# print(y_pred.shape)\n",
    "# print(y_proba.shape)\n",
    "\n",
    "nacc_results = pd.DataFrame(nacc_theta, columns=[f\"Topic_{k}\" for k in range(lda.n_topics)])\n",
    "print(nacc_results.shape)\n",
    "\n",
    "subj_col = nacc_prep.subject_col\n",
    "if subj_col in nacc_pat.columns:\n",
    "    nacc_results.insert(0, \"SUBJ_ID\", nacc_pat[subj_col].values)\n",
    "\n",
    "nacc_results['pred_DX'] = y_pred\n",
    "for i, dx in enumerate(classifier.classes):\n",
    "    nacc_results[f\"P({dx})\"] = y_proba[:,i]\n",
    "\n",
    "nacc_results = nacc_results.merge(\n",
    "    df_nacc_resilience[[\"SUBJ_ID\", \"TN_group\"]],\n",
    "    on=\"SUBJ_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "nacc_results = nacc_results.dropna(subset=['TN_group'])\n",
    "\n",
    "nacc_results = nacc_results.merge(\n",
    "    nacc_raw[[\"subject_id\", \"DX\"]],\n",
    "    left_on=\"SUBJ_ID\",\n",
    "    right_on=\"subject_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "nacc_results = nacc_results.drop(columns=[\"subject_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8abdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "from itertools import cycle\n",
    "def plot_dx_radar(df, dx_col=\"DX\", topic_prefix=\"Topic_\", min_n=5, ncols=4):\n",
    "    topic_cols = [c for c in df.columns if c.startswith(topic_prefix)]\n",
    "    angles = np.linspace(0, 2*np.pi, len(topic_cols), endpoint=False)\n",
    "    angles = np.r_[angles, angles[0]]\n",
    "\n",
    "    # valid DX groups\n",
    "    groups = [\n",
    "        (dx, (df[dx_col] == dx).sum())\n",
    "        for dx in sorted(df[dx_col].dropna().unique())\n",
    "        if (df[dx_col] == dx).sum() >= min_n\n",
    "    ]\n",
    "\n",
    "    if not groups:\n",
    "        print(\"No DX groups meet minimum subject requirement.\")\n",
    "        return\n",
    "\n",
    "    def profile(d):\n",
    "        v = d[topic_cols].mean().values\n",
    "        return np.r_[v, v[0]]\n",
    "\n",
    "    colors = plt.cm.tab10.colors\n",
    "\n",
    "    # ===============================\n",
    "    # Individual radar panels\n",
    "    # ===============================\n",
    "    nrows = ceil(len(groups) / ncols)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, ncols,\n",
    "        figsize=(4*ncols, 4*nrows),\n",
    "        subplot_kw=dict(polar=True)\n",
    "    )\n",
    "\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    color_cycle = cycle(colors)\n",
    "\n",
    "    for ax, (dx, n) in zip(axes, groups):\n",
    "        vals = profile(df[df[dx_col] == dx])\n",
    "        c = next(color_cycle)\n",
    "\n",
    "        ax.plot(angles, vals, lw=2.5, color=c)\n",
    "        ax.fill(angles, vals, alpha=0.25, color=c)\n",
    "        ax.set_thetagrids(angles[:-1]*180/np.pi, topic_cols)\n",
    "        ax.set_title(f\"{dx}\\n(n={n})\", fontsize=11)\n",
    "\n",
    "    for ax in axes[len(groups):]:\n",
    "        ax.remove()\n",
    "\n",
    "    plt.suptitle(\"Topic Profiles by Diagnosis\", y=1.02, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # # ===============================\n",
    "    # # Combined overlay\n",
    "    # # ===============================\n",
    "    # plt.figure(figsize=(8, 8))\n",
    "    # ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # for dx, n in groups:\n",
    "    #     vals = profile(df[df[dx_col] == dx])\n",
    "    #     c = next(cycle(colors))\n",
    "    #     ax.plot(angles, vals, lw=2.5, color=c)\n",
    "    #     ax.fill(angles, vals, alpha=0.12, color=c)\n",
    "\n",
    "    # ax.set_thetagrids(angles[:-1]*180/np.pi, topic_cols)\n",
    "    # ax.set_title(\"Combined Topic Profiles\", pad=25)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "plot_dx_radar(nacc_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pathology-wise Radar Plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil\n",
    "from itertools import cycle\n",
    "\n",
    "viz_df = nacc_results.merge(\n",
    "    nacc_pat[[nacc_prep.subject_col] + list(pathology_cols)],\n",
    "    left_on=\"SUBJ_ID\",\n",
    "    right_on=nacc_prep.subject_col,\n",
    "    how=\"left\"\n",
    ")\n",
    "# -----------------------------\n",
    "# Topic label map\n",
    "# -----------------------------\n",
    "\n",
    "topic_cols = [c for c in viz_df.columns if c.startswith('Topic_')]\n",
    "\n",
    "# -----------------------------\n",
    "# Radar utilities\n",
    "# -----------------------------\n",
    "def compute_topic_profile(df):\n",
    "    values = df[topic_cols].mean().values\n",
    "    return np.concatenate([values, [values[0]]])\n",
    "\n",
    "\n",
    "def radar_angles(n):\n",
    "    angles = np.linspace(0, 2 * np.pi, n, endpoint=False)\n",
    "    return np.concatenate([angles, [angles[0]]])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main visualization\n",
    "# -----------------------------\n",
    "def plot_pathology_radar_panels(\n",
    "    viz_df,\n",
    "    pathology_cols,\n",
    "    min_subjects=1,\n",
    "    ncols=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates:\n",
    "    1) Grid of individual pathology radar plots\n",
    "    2) One large combined radar plot\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # collect valid pathologies\n",
    "    # -------------------------\n",
    "    valid = []\n",
    "    for p in pathology_cols:\n",
    "        n = (viz_df[p] == 1).sum()\n",
    "        if n >= min_subjects:\n",
    "            valid.append((p, n))\n",
    "\n",
    "    if len(valid) == 0:\n",
    "        print(\"No pathology columns with positive subjects.\")\n",
    "        return\n",
    "\n",
    "    angles = radar_angles(len(topic_cols))\n",
    "    colors = plt.cm.tab10.colors\n",
    "    color_cycle = cycle(colors)\n",
    "\n",
    "    # ======================================================\n",
    "    # 1. INDIVIDUAL RADAR SUBPLOTS\n",
    "    # ======================================================\n",
    "    n_panels = len(valid)\n",
    "    nrows = ceil(n_panels / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows,\n",
    "        ncols=ncols,\n",
    "        figsize=(4 * ncols, 4 * nrows),\n",
    "        subplot_kw=dict(polar=True)\n",
    "    )\n",
    "\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for ax, (pathology, n_pos) in zip(axes, valid):\n",
    "\n",
    "        pos_df = viz_df[viz_df[pathology] == 1]\n",
    "        values = compute_topic_profile(pos_df)\n",
    "        color = next(color_cycle)\n",
    "\n",
    "        ax.plot(angles, values, linewidth=2.5, color=color)\n",
    "        ax.fill(angles, values, alpha=0.25, color=color)\n",
    "\n",
    "        ax.set_thetagrids(\n",
    "            angles[:-1] * 180 / np.pi,\n",
    "            topic_cols,\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"{pathology}\\n(n={n_pos})\", fontsize=11, pad=12)\n",
    "\n",
    "    # remove unused axes\n",
    "    for ax in axes[len(valid):]:\n",
    "        ax.remove()\n",
    "\n",
    "    fig.suptitle(\n",
    "        \"Topic Expression by Copathology (Positive Subjects Only)\",\n",
    "        fontsize=16,\n",
    "        y=1.02\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ======================================================\n",
    "    # 2. COMBINED RADAR OVERLAY\n",
    "    # ======================================================\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    legend_labels = []\n",
    "    color_cycle = cycle(colors)\n",
    "\n",
    "    for pathology, n_pos in valid:\n",
    "\n",
    "        pos_df = viz_df[viz_df[pathology] == 1]\n",
    "        values = compute_topic_profile(pos_df)\n",
    "        color = next(color_cycle)\n",
    "\n",
    "        ax.plot(angles, values, linewidth=2.5, color=color)\n",
    "        ax.fill(angles, values, alpha=0.12, color=color)\n",
    "\n",
    "        legend_labels.append(f\"{pathology} (n={n_pos})\")\n",
    "\n",
    "    ax.set_thetagrids(\n",
    "        angles[:-1] * 180 / np.pi,\n",
    "        topic_cols,\n",
    "        fontsize=12\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        \"Combined Topic Profiles Across Copathologies\",\n",
    "        fontsize=16,\n",
    "        pad=30\n",
    "    )\n",
    "\n",
    "    ax.legend(\n",
    "        legend_labels,\n",
    "        bbox_to_anchor=(1.35, 1.1),\n",
    "        loc=\"upper right\",\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_pathology_radar_panels(\n",
    "    viz_df=viz_df,\n",
    "    pathology_cols=pathology_cols,\n",
    "    ncols=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4822cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# prob_cols = nacc_results.loc[:,'P(AD)':'P(svPPA)'].columns\n",
    "# nacc_results = nacc_results[nacc_results['DX']=='AD']######## TEMP\n",
    "prob_cols = nacc_results.loc[:,'P(AD)':'P(SVAD)'].columns\n",
    "group_col = 'TN_group'\n",
    "# ------------------------------------------------------------\n",
    "# Compute group-wise mean probabilities\n",
    "# ------------------------------------------------------------\n",
    "group_means = (\n",
    "    nacc_results\n",
    "    .groupby('TN_group')[prob_cols]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot heatmap\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    group_means,\n",
    "    cmap=\"Reds\",\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    "    vmin=0,\n",
    "    vmax=0.4,\n",
    "    cbar_kws={\"label\": \"Mean predicted probability\"}\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Predicted pathology\")\n",
    "plt.ylabel(\"Subgroup\")\n",
    "plt.title(\"Group-wise Mean Predicted Probability Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# prob_cols = nacc_results.loc[:, 'P(AD)':'P(svPPA)'].columns\n",
    "prob_cols = nacc_results.loc[:, 'P(AD)':'P(SVAD)'].columns\n",
    "group_col = \"TN_group\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sort: group first, then descending P(AD)\n",
    "# ------------------------------------------------------------\n",
    "nacc_sorted = (\n",
    "    nacc_results\n",
    "    .sort_values([group_col, \"P(AD)\"], ascending=[True, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "heatmap_data = nacc_sorted[prob_cols]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute group positions for y-axis labels\n",
    "# ------------------------------------------------------------\n",
    "group_counts = nacc_sorted[group_col].value_counts(sort=False)\n",
    "\n",
    "group_centers = {}\n",
    "start = 0\n",
    "\n",
    "for grp, count in group_counts.items():\n",
    "    center = start + count / 2\n",
    "    group_centers[grp] = center\n",
    "    start += count\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap=\"Reds\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    yticklabels=False,\n",
    "    cbar_kws={\"label\": \"Predicted probability\"}\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Horizontal lines between groups\n",
    "# ------------------------------------------------------------\n",
    "cum_sizes = np.cumsum(group_counts.values)\n",
    "\n",
    "for y in cum_sizes[:-1]:\n",
    "    ax.hlines(y, *ax.get_xlim(), colors=\"black\", linewidth=1.5)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TN subgroup labels on y-axis\n",
    "# ------------------------------------------------------------\n",
    "ax.set_yticks(list(group_centers.values()))\n",
    "ax.set_yticklabels(list(group_centers.keys()), rotation=0, fontsize=11)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Labels\n",
    "# ------------------------------------------------------------\n",
    "ax.set_xlabel(\"Predicted pathology\")\n",
    "ax.set_ylabel(\"TN subgroup\")\n",
    "ax.set_title(\"Subject-level Predicted Probability Heatmap\\n(sorted by descending P(AD))\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topic_cols = [c for c in nacc_results.columns if c.startswith(\"Topic_\")]\n",
    "group_col = \"TN_group\"\n",
    "\n",
    "groups = nacc_results[group_col].unique()\n",
    "n_groups = len(groups)\n",
    "n_topics = len(topic_cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global max for shared axis\n",
    "# ------------------------------------------------------------\n",
    "global_max = (\n",
    "    nacc_results\n",
    "    .groupby(group_col)[topic_cols]\n",
    "    .mean()\n",
    "    .values\n",
    "    .max()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Radar setup\n",
    "# ------------------------------------------------------------\n",
    "angles = np.linspace(0, 2 * np.pi, n_topics, endpoint=False)\n",
    "angles = np.concatenate([angles, [angles[0]]])\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, n_groups,\n",
    "    figsize=(4 * n_groups, 4),\n",
    "    subplot_kw=dict(polar=True)\n",
    ")\n",
    "\n",
    "if n_groups == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot\n",
    "# ------------------------------------------------------------\n",
    "for ax, grp in zip(axes, groups):\n",
    "\n",
    "    grp_df = nacc_results[nacc_results[group_col] == grp]\n",
    "    mean_topics = grp_df[topic_cols].mean().values\n",
    "    mean_topics = np.concatenate([mean_topics, [mean_topics[0]]])\n",
    "\n",
    "    ax.plot(angles, mean_topics, linewidth=2)\n",
    "    ax.fill(angles, mean_topics, alpha=0.25)\n",
    "\n",
    "    ax.set_title(grp, pad=20)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(topic_cols, fontsize=9)\n",
    "\n",
    "    ax.set_ylim(0, global_max * 1.1)   # âœ… shared scale\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "plt.suptitle(\"TN Subgroup Topic Weight Profiles (shared radial scale)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fe958",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NACC vulnerable check\n",
    "print('VULN')\n",
    "r2 = nacc_results[nacc_results['TN_group']=='Vulnerable']\n",
    "print(r2['DX'].value_counts())\n",
    "print('\\nRESIL')\n",
    "r2 = nacc_results[nacc_results['TN_group']=='Resilient']\n",
    "print(r2['DX'].value_counts())\n",
    "print('\\nCANON')\n",
    "r2 = nacc_results[nacc_results['TN_group']=='Canonical']\n",
    "print(r2['DX'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4a573",
   "metadata": {},
   "source": [
    "**ADNI INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADNI4 Inference ## 260120\n",
    "adni4_raw = pd.read_csv('C:/Users/BREIN/Desktop/copathology_visualization_temp/data/stage_data/ptau_volume_model/ADNI4_3.csv')\n",
    "region_cols = adni4_raw.loc[:, 'VA/2':'VA/2035'].columns\n",
    "adni4_raw = adni4_raw.dropna(subset=region_cols)\n",
    "adni4_cn = adni4_raw[adni4_raw['DX'] == 'CN']\n",
    "adni4_pat = adni4_raw[adni4_raw['DX'] != 'CN']\n",
    "adni4_stage_df = adni4_pat[['FULL_ID', 'DX', 'tau_stage_aa/low', 'tau_stage_aa/mid', 'tau_stage_aa/high', 'pred_tau_stage_aa/low', 'pred_tau_stage_aa/mid', 'pred_tau_stage_aa/high']].dropna()\n",
    "prob_cols = ['pred_tau_stage_aa/low','pred_tau_stage_aa/mid','pred_tau_stage_aa/high']\n",
    "max_col = adni4_stage_df[prob_cols].idxmax(axis=1)\n",
    "adni4_stage_df[prob_cols] = 0\n",
    "adni4_stage_df.loc[:, prob_cols] = (pd.get_dummies(max_col).reindex(columns=prob_cols, fill_value=0).astype(float))\n",
    "stage_map = {\n",
    "    'low': 0,\n",
    "    'mid': 1,\n",
    "    'high': 2\n",
    "}\n",
    "def get_stage(colname):\n",
    "    return stage_map[colname.split('/')[-1]]\n",
    "adni4_stage_df['gt_stage'] = (\n",
    "    adni4_stage_df[['tau_stage_aa/low', 'tau_stage_aa/mid', 'tau_stage_aa/high']]\n",
    "    .idxmax(axis=1)\n",
    "    .apply(get_stage)\n",
    ")\n",
    "\n",
    "adni4_stage_df['pred_stage'] = (\n",
    "    adni4_stage_df[prob_cols]\n",
    "    .idxmax(axis=1)\n",
    "    .apply(get_stage)\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# Subject grouping\n",
    "# --------------------------------\n",
    "adni4_lower_than_pred = adni4_stage_df[\n",
    "    adni4_stage_df['gt_stage'] < adni4_stage_df['pred_stage']\n",
    "][['FULL_ID', 'DX', 'gt_stage', 'pred_stage']]\n",
    "\n",
    "adni4_exact_match = adni4_stage_df[\n",
    "    adni4_stage_df['gt_stage'] == adni4_stage_df['pred_stage']\n",
    "][['FULL_ID', 'DX', 'gt_stage', 'pred_stage']]\n",
    "\n",
    "\n",
    "\n",
    "adni4_prep = DataProcessor(\n",
    "    region_cols=region_cols,\n",
    "    dx_col='DX',\n",
    "    subject_col='FULL_ID'\n",
    ")\n",
    "adni4_prep.fit_baseline(hc_data=adni4_cn)\n",
    "adni4_Z = adni4_prep.compute_atrophy_scores(data=adni4_pat)\n",
    "print(adni4_Z.shape)\n",
    "print(adni4_cn.shape)\n",
    "\n",
    "adni4_theta = lda.transform(adni4_Z)\n",
    "adni4_y_pred = classifier.predict(adni4_theta)\n",
    "adni4_y_proba = classifier.predict_proba(adni4_theta)\n",
    "\n",
    "adni4_results = pd.DataFrame(adni4_theta, columns=[f\"Topic_{k}\" for k in range(lda.n_topics)])\n",
    "print(adni4_results.shape)\n",
    "\n",
    "subj_col = adni4_prep.subject_col\n",
    "if subj_col in adni4_pat.columns:\n",
    "    adni4_results.insert(0, \"SUBJ_ID\", adni4_pat[subj_col].values)\n",
    "\n",
    "adni4_results['pred_DX'] = adni4_y_pred\n",
    "for i, dx in enumerate(classifier.classes):\n",
    "    adni4_results[f\"P({dx})\"] = adni4_y_proba[:,i]\n",
    "\n",
    "adni4_results = adni4_results.merge(\n",
    "    adni4_raw[[\"FULL_ID\", \"DX\"]],\n",
    "    left_on=\"SUBJ_ID\",\n",
    "    right_on=\"FULL_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "adni4_results = adni4_results.drop(columns=[\"FULL_ID\"])\n",
    "\n",
    "adni4_results[\"SUBJ_ID\"] = adni4_results[\"SUBJ_ID\"].str.replace(\"_M\", \"_m\", regex=False)\n",
    "\n",
    "adni4_results = adni4_results.merge(\n",
    "    df_adni4_resilience[[\"SUBJ_ID\", \"TN_group\"]],\n",
    "    on=\"SUBJ_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "adni4_results = adni4_results.dropna(subset=['TN_group'])\n",
    "print('!!!!', adni4_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bca3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dx_radar(adni4_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "adni4_results = adni4_results[adni4_results['DX']=='AD']######## TEMP\n",
    "\n",
    "prob_cols = adni4_results.loc[:,'P(AD)':'P(bvFTD)'].columns\n",
    "\n",
    "group_col = 'TN_group'\n",
    "# ------------------------------------------------------------\n",
    "# Compute group-wise mean probabilities\n",
    "# ------------------------------------------------------------\n",
    "group_means = (\n",
    "    adni4_results\n",
    "    .groupby('TN_group')[prob_cols]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot heatmap\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    group_means,\n",
    "    cmap=\"Reds\",\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    "    vmin=0,\n",
    "    vmax=0.4,\n",
    "    cbar_kws={\"label\": \"Mean predicted probability\"}\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Predicted pathology\")\n",
    "plt.ylabel(\"Subgroup\")\n",
    "plt.title(\"ADNI4 Group-wise Mean Predicted Probability Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eda95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_cols = adni4_results.loc[:, 'P(AD)':'P(bvFTD)'].columns\n",
    "group_col = \"TN_group\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sort: group first, then descending P(AD)\n",
    "# ------------------------------------------------------------\n",
    "adni4_sorted = (\n",
    "    adni4_results\n",
    "    .sort_values([group_col, \"P(AD)\"], ascending=[True, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "heatmap_data = adni4_sorted[prob_cols]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute group positions for y-axis labels\n",
    "# ------------------------------------------------------------\n",
    "group_counts = adni4_sorted[group_col].value_counts(sort=False)\n",
    "\n",
    "group_centers = {}\n",
    "start = 0\n",
    "\n",
    "for grp, count in group_counts.items():\n",
    "    center = start + count / 2\n",
    "    group_centers[grp] = center\n",
    "    start += count\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap=\"Reds\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    yticklabels=False,\n",
    "    cbar_kws={\"label\": \"Predicted probability\"}\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Horizontal lines between groups\n",
    "# ------------------------------------------------------------\n",
    "cum_sizes = np.cumsum(group_counts.values)\n",
    "\n",
    "for y in cum_sizes[:-1]:\n",
    "    ax.hlines(y, *ax.get_xlim(), colors=\"black\", linewidth=1.5)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TN subgroup labels on y-axis\n",
    "# ------------------------------------------------------------\n",
    "ax.set_yticks(list(group_centers.values()))\n",
    "ax.set_yticklabels(list(group_centers.keys()), rotation=0, fontsize=11)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Labels\n",
    "# ------------------------------------------------------------\n",
    "ax.set_xlabel(\"Predicted pathology\")\n",
    "ax.set_ylabel(\"TN subgroup\")\n",
    "ax.set_title(\"ADNI4 Subject-level Predicted Probability Heatmap\\n(sorted by descending P(AD))\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef694e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topic_cols = [c for c in adni4_results.columns if c.startswith(\"Topic_\")]\n",
    "group_col = \"TN_group\"\n",
    "\n",
    "groups = adni4_results[group_col].unique()\n",
    "n_groups = len(groups)\n",
    "n_topics = len(topic_cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global max for shared axis\n",
    "# ------------------------------------------------------------\n",
    "global_max = (\n",
    "    adni4_results\n",
    "    .groupby(group_col)[topic_cols]\n",
    "    .mean()\n",
    "    .values\n",
    "    .max()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Radar setup\n",
    "# ------------------------------------------------------------\n",
    "angles = np.linspace(0, 2 * np.pi, n_topics, endpoint=False)\n",
    "angles = np.concatenate([angles, [angles[0]]])\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, n_groups,\n",
    "    figsize=(4 * n_groups, 4),\n",
    "    subplot_kw=dict(polar=True)\n",
    ")\n",
    "\n",
    "if n_groups == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot\n",
    "# ------------------------------------------------------------\n",
    "for ax, grp in zip(axes, groups):\n",
    "\n",
    "    grp_df = adni4_results[adni4_results[group_col] == grp]\n",
    "    mean_topics = grp_df[topic_cols].mean().values\n",
    "    mean_topics = np.concatenate([mean_topics, [mean_topics[0]]])\n",
    "\n",
    "    ax.plot(angles, mean_topics, linewidth=2)\n",
    "    ax.fill(angles, mean_topics, alpha=0.25)\n",
    "\n",
    "    ax.set_title(grp, pad=20)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(topic_cols, fontsize=9)\n",
    "\n",
    "    ax.set_ylim(0, global_max * 1.1)   # âœ… shared scale\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "plt.suptitle(\"TN Subgroup Topic Weight Profiles (shared radial scale)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADNI4 vulnerable check\n",
    "print('VULN')\n",
    "r2 = adni4_results[adni4_results['TN_group']=='Vulnerable']\n",
    "print(r2['DX'].value_counts())\n",
    "print('\\nRESIL')\n",
    "r2 = adni4_results[adni4_results['TN_group']=='Resilient']\n",
    "print(r2['DX'].value_counts())\n",
    "print('\\nCANON')\n",
    "r2 = adni4_results[adni4_results['TN_group']=='Canonical']\n",
    "print(r2['DX'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_management",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
