{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nacc_raw = nacc_raw.merge(\n",
    "#     df_nacc_resilience[[\"SUBJ_ID\", \"TN_group\"]],\n",
    "#     left_on=\"subject_id\",\n",
    "#     right_on='SUBJ_ID',\n",
    "#     how=\"left\"\n",
    "# )\n",
    "# nacc_raw = nacc_raw.dropna(subset=['TN_group'])\n",
    "\n",
    "\n",
    "## NACc Check Copathology\n",
    "path_cols = [\n",
    "    \"NACC_IMCI\",\n",
    "    \"NACC_MCI\",\n",
    "    \"NACC_AD\",\n",
    "    \"NACC_PD\",\n",
    "    \"NACC_LBD\",\n",
    "    \"NACC_VD\",\n",
    "    \"NACC_PCA\",\n",
    "    \"NACC_FTD_ANY\"\n",
    "]\n",
    "\n",
    "ad_cols = [\"NACC_IMCI\", \"NACC_MCI\", \"NACC_AD\"]\n",
    "non_ad_cols = [c for c in path_cols if c not in ad_cols]\n",
    "\n",
    "\n",
    "def assign_pathology(row):\n",
    "\n",
    "    ad_present = any(row[c] == 1 for c in ad_cols)\n",
    "    non_ad_pos = [c.replace(\"NACC_\", \"\") for c in non_ad_cols if row[c] == 1]\n",
    "\n",
    "    # -------------------------\n",
    "    # Pure AD spectrum\n",
    "    # -------------------------\n",
    "    if ad_present and len(non_ad_pos) == 0:\n",
    "        return \"AD_SPECTRUM\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Mixed AD + others\n",
    "    # -------------------------\n",
    "    if ad_present and len(non_ad_pos) > 0:\n",
    "        return \"AD+\" + \"+\".join(sorted(non_ad_pos))\n",
    "\n",
    "    # -------------------------\n",
    "    # Non-AD only\n",
    "    # -------------------------\n",
    "    if not ad_present and len(non_ad_pos) > 0:\n",
    "        return \"+\".join(sorted(non_ad_pos))\n",
    "\n",
    "    # -------------------------\n",
    "    # No pathology\n",
    "    # -------------------------\n",
    "    return \"None\"\n",
    "\n",
    "\n",
    "nacc_raw[\"PATH_LABEL\"] = nacc_raw.apply(assign_pathology, axis=1)\n",
    "\n",
    "print(nacc_raw[\"PATH_LABEL\"].value_counts())\n",
    "\n",
    "\n",
    "def collapse_train_group(path_label):\n",
    "    \n",
    "    if path_label == \"AD_SPECTRUM\":\n",
    "        return \"AD_PURE\"\n",
    "\n",
    "    if path_label.startswith(\"AD+\"):\n",
    "        return \"AD_MIXED\"\n",
    "\n",
    "    if path_label == \"None\":\n",
    "        return \"None\"\n",
    "\n",
    "    return \"OTHER\"\n",
    "\n",
    "\n",
    "nacc_raw[\"TRAIN_GROUP\"] = nacc_raw[\"PATH_LABEL\"].apply(collapse_train_group)\n",
    "\n",
    "print(nacc_raw[\"TRAIN_GROUP\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f97a0aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlda_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LDATopicModel\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclassifier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TopicClassifier\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvisualizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbrain_visualizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\WooSikKim\\Desktop\\Research\\projects\\co_pathology\\scripts\\stage_copath\\STAGE_CoPathology\\LDA_XGB\\classifier.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Dict, List, Any\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_processor import *\n",
    "from lda_model import LDATopicModel\n",
    "from classifier import TopicClassifier\n",
    "from visualizer import *\n",
    "from brain_visualizer import *\n",
    "\n",
    "data_path = 'C:/Users/WooSikKim/Desktop/Research/projects/co_pathology/scripts/stage_copath/data'\n",
    "inp_df = pd.read_csv(data_path + '/260120_wsev_smc_combined_zscores.csv')\n",
    "# inp_df = inp_df[~inp_df[\"DX\"].isin(['svPPA', 'nfvPPA'])] #######################\n",
    "print(inp_df['DX'].value_counts())\n",
    "\n",
    "\n",
    "df_nacc_resilience = pd.read_csv(data_path + '/nacc/NACC_resilience_inference.csv')\n",
    "df_adni4_resilience = pd.read_csv(data_path + '/adni/ADNI4_resilience_inference.csv')\n",
    "df_nacc_resilience = pd.read_csv('C:/Users/BREIN/Desktop/stage_copath/20260122_NACC_linear_group.csv')\n",
    "df_adni4_resilience = df_adni4_resilience.rename(columns={\"FULL_ID\": \"SUBJ_ID\"})\n",
    "df_nacc_resilience = df_nacc_resilience.rename(columns={\"subject_id\" : \"SUBJ_ID\"})\n",
    "nacc_raw = pd.read_csv(data_path + '/nacc/260120_NACC_VA_TAU_PATH_matched.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAMPLE MCI FROM NACC AND ADD TO TRAIN DATA ##\n",
    "mci_candidates = nacc_raw[\n",
    "    (nacc_raw[\"NACC_MCI\"] == 1) &  # MCI positive\n",
    "    (nacc_raw['DX'] == 'MCI') &\n",
    "    (~nacc_raw[\"subject_id\"].isin(df_nacc_resilience[\"FULL_ID\"]))  # not already in resilience\n",
    "]\n",
    "print(f\"Total MCI candidates outside resilience: {len(mci_candidates)}\")\n",
    "N = 25  # number you want to add\n",
    "mci_sample = mci_candidates.sample(n=min(N, len(mci_candidates)), random_state=42)\n",
    "\n",
    "region_cols = nacc_raw.loc[:, 'VA/2':'VA/2035'].columns\n",
    "mci_prep = DataProcessor(\n",
    "    region_cols=region_cols,\n",
    "    dx_col='DX',\n",
    "    subject_col='subject_id'\n",
    ")\n",
    "region_cols = nacc_raw.loc[:, 'VA/2':'VA/2035'].columns\n",
    "pathology_cols = nacc_raw.loc[:, 'NACC_AD':'NACC_svPPA'].columns\n",
    "nacc_filtered = nacc_raw[nacc_raw['DX'] != 'Unknown']\n",
    "\n",
    "nacc_cn = nacc_filtered[nacc_filtered['DX'] == 'CN']\n",
    "\n",
    "mci_prep.fit_baseline(hc_data=nacc_cn)\n",
    "nacc_Z = mci_prep.compute_atrophy_scores(data=mci_sample)\n",
    "print(type(nacc_Z))\n",
    "print(nacc_Z.shape)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the Z-scores array to a DataFrame\n",
    "nacc_Z_df = pd.DataFrame(nacc_Z, columns=region_cols)\n",
    "\n",
    "# Add subject IDs\n",
    "nacc_Z_df['SUBJ_ID'] = mci_sample['subject_id'].values\n",
    "\n",
    "# Optional: add DX if you want\n",
    "nacc_Z_df['DX'] = mci_sample['DX'].values\n",
    "\n",
    "# Now you can concatenate\n",
    "inp_df = pd.concat([inp_df, nacc_Z_df], ignore_index=True)\n",
    "print(f\"New training dataset shape: {inp_df.shape}\")\n",
    "\n",
    "print(inp_df['DX'].value_counts())\n",
    "\n",
    "## DOWNSAMPLE LARGE DX \n",
    "N = 25\n",
    "dx_col = \"DX\"\n",
    "balanced_parts = []\n",
    "\n",
    "for dx, g in inp_df.groupby(dx_col):\n",
    "    # if dx == 'AD':\n",
    "    #     N=50\n",
    "    # else: \n",
    "    #     N=25\n",
    "    if len(g) > N:\n",
    "        g = g.sample(n=N, replace=False, random_state=42)\n",
    "    balanced_parts.append(g)\n",
    "\n",
    "balanced_df = pd.concat(balanced_parts).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#### add mci to AD ####\n",
    "# balanced_df['DX'] = balanced_df['DX'].replace({'MCI' : 'AD'})\n",
    "\n",
    "print(balanced_df[dx_col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c69a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in list(range(6, 25, 2)):\n",
    "n=18\n",
    "# for n in [10,12,14,16,18,20,22]:\n",
    "print('k_topics = ', n)\n",
    "N_TOPICS = n ###\n",
    "region_cols = list(balanced_df.loc[:, \"VA/2\":\"VA/2035\"].columns)\n",
    "labels = balanced_df[\"DX\"].values\n",
    "ids = balanced_df[\"SUBJ_ID\"].values\n",
    "\n",
    "# Fit LDA on combined z-scores\n",
    "lda = LDATopicModel(n_topics=N_TOPICS)\n",
    "theta = lda.fit_transform(balanced_df[region_cols])\n",
    "\n",
    "# Fit classifier\n",
    "classifier = TopicClassifier(n_splits=5) ##\n",
    "cv_results = classifier.cross_validate(theta, labels, ids, verbose=False)\n",
    "classifier.fit(theta, labels)\n",
    "\n",
    "print(f\"K_topics {n}, CV Accuracy: {cv_results['accuracy']:.4f}\")\n",
    "\n",
    "# visualizer = CopathologyVisualizer(\n",
    "#     output_dir=f'./train_mci_added/topics_{N_TOPICS}_downsampled'\n",
    "# )\n",
    "\n",
    "# fig_conf_mat = visualizer.plot_confusion_matrix(\n",
    "#     cm=classifier.get_confusion_matrix(),\n",
    "#     class_names=classifier._classes\n",
    "# )\n",
    "\n",
    "# fig_top_regions = visualizer.plot_top_regions_per_topic(\n",
    "#     topic_patterns = lda.get_topic_patterns(),\n",
    "#     region_names=region_cols\n",
    "# )\n",
    "\n",
    "# fig3 = visualizer.plot_diagnosis_topic_profiles(\n",
    "#     theta=lda._theta,\n",
    "#     dx_labels = labels\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed74edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda._theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb704a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Surface Mapping \n",
    "## Topicwise Surface Maps\n",
    "from atlas_vis import DKTAtlas62ROIPlotter\n",
    "plotter_62  = DKTAtlas62ROIPlotter(\n",
    "    cmap='Reds',\n",
    "    clim=(0, 0.1),  \n",
    "    window_size=(1200, 1000),\n",
    "    nan_color='lightgray',\n",
    "    background='white',\n",
    "    template_key='pial'\n",
    ")\n",
    "os.makedirs(f'./train_mci_added/topics_{n}_downsampled/topicwise',exist_ok=True)\n",
    "\n",
    "topic_df = pd.DataFrame(\n",
    "    lda.get_topic_patterns().T,\n",
    "    index=region_cols,\n",
    "    columns=[f\"Topic_{k}\" for k in range(n)]\n",
    ")\n",
    "\n",
    "df = topic_df.tail(62).reset_index(drop=True)\n",
    "\n",
    "print(len(df))\n",
    "for col in df.columns: ##################\n",
    "    print(col)\n",
    "    l_values = df.loc[:30,col].to_list()\n",
    "    r_values = df.loc[31:,col].to_list()\n",
    "    print(len(l_values))\n",
    "    print(len(r_values))\n",
    "    print(np.min(l_values+r_values))\n",
    "    print(np.max(l_values+r_values))\n",
    "\n",
    "    plotter_62(l_values, r_values, save_path=f'./train_mci_added/topics_{n}_downsampled/topicwise/{col}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d33d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NACC Inference ## 260120\n",
    "from data_processor import *\n",
    "nacc_raw = pd.read_csv('C:/Users/BREIN/Desktop/stage_copath/data/nacc/260120_NACC_VA_TAU_PATH_matched.csv')\n",
    "# nacc_raw.rename(columns=col_map)\n",
    "region_cols = nacc_raw.loc[:, 'VA/2':'VA/2035'].columns\n",
    "pathology_cols = nacc_raw.loc[:, 'NACC_AD':'NACC_svPPA'].columns\n",
    "nacc_filtered = nacc_raw[nacc_raw['DX'] != 'Unknown']\n",
    "\n",
    "nacc_cn = nacc_filtered[nacc_filtered['DX'] == 'CN']\n",
    "nacc_pat = nacc_filtered[nacc_filtered['DX'] != 'CN']\n",
    "# nacc_pat = nacc_filtered\n",
    "\n",
    "print(nacc_cn.shape)\n",
    "print(nacc_pat.shape)\n",
    "\n",
    "nacc_prep = DataProcessor(\n",
    "    region_cols=region_cols,\n",
    "    dx_col='DX',\n",
    "    subject_col='subject_id'\n",
    ")\n",
    "nacc_prep.fit_baseline(hc_data=nacc_cn)\n",
    "nacc_Z = nacc_prep.compute_atrophy_scores(data=nacc_pat)\n",
    "print(type(nacc_Z))\n",
    "\n",
    "nacc_theta = lda.transform(nacc_Z)\n",
    "y_pred = classifier.predict(nacc_theta)\n",
    "y_proba = classifier.predict_proba(nacc_theta)\n",
    "# print(nacc_theta.shape)\n",
    "# print(y_pred.shape)\n",
    "# print(y_proba.shape)\n",
    "\n",
    "nacc_results = pd.DataFrame(nacc_theta, columns=[f\"Topic_{k}\" for k in range(lda.n_topics)])\n",
    "print(nacc_results.shape)\n",
    "\n",
    "subj_col = nacc_prep.subject_col\n",
    "if subj_col in nacc_pat.columns:\n",
    "    nacc_results.insert(0, \"SUBJ_ID\", nacc_pat[subj_col].values)\n",
    "\n",
    "nacc_results['pred_DX'] = y_pred\n",
    "for i, dx in enumerate(classifier.classes):\n",
    "    nacc_results[f\"P({dx})\"] = y_proba[:,i]\n",
    "\n",
    "# nacc_results = nacc_results.merge(\n",
    "#     df_nacc_resilience[[\"SUBJ_ID\", \"TN_group\", \"standardized_residual\"]],\n",
    "#     on=\"SUBJ_ID\",\n",
    "#     how=\"left\"\n",
    "# )\n",
    "# nacc_results = nacc_results.dropna(subset=['TN_group'])\n",
    "\n",
    "nacc_results = nacc_results.merge(\n",
    "    df_nacc_resilience[[\"FULL_ID\", \"linear_group\", \"standardized_residual\"]],\n",
    "    left_on=\"SUBJ_ID\",\n",
    "    right_on='FULL_ID',\n",
    "    how=\"left\"\n",
    ")\n",
    "nacc_results = nacc_results.dropna(subset=['linear_group'])\n",
    "\n",
    "nacc_results = nacc_results.merge(\n",
    "    nacc_raw[[\"subject_id\", \"DX\", 'NACC_AD', 'NACC_PD', 'NACC_VD', 'NACC_LBD', 'NACC_SVAD', 'NACC_PCA', 'NACC_bvFTD']],\n",
    "    left_on=\"SUBJ_ID\",\n",
    "    right_on=\"subject_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "nacc_results = nacc_results.drop(columns=[\"subject_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf34939",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TN Group Margin Chage ##\n",
    "# Example column: 'standardized_residual'\n",
    "conditions = [\n",
    "    nacc_results['standardized_residual'] > 1,\n",
    "    nacc_results['standardized_residual'] < -1\n",
    "]\n",
    "\n",
    "choices = [\n",
    "    'Vulnerable',\n",
    "    'Resilient'\n",
    "]\n",
    "\n",
    "# Default is 'canonical'\n",
    "nacc_results['TN_group_1'] = np.select(conditions, choices, default='Canonical')\n",
    "\n",
    "# Quick check\n",
    "print(nacc_results[['standardized_residual', 'TN_group_1']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_cols = nacc_results.loc[:,'P(AD)':'P(svPPA)'].columns\n",
    "prob_cols = ['P(AD)', 'P(MCI)', 'P(PD)', 'P(DLB)', 'P(SVAD)', 'P(bvFTD)', 'P(nfvPPA)', 'P(svPPA)']\n",
    "# nacc_results = nacc_results[nacc_results['DX']!='IMCI']######## TEMP\n",
    "# prob_cols = nacc_results.loc[:,'P(AD)':'P(bvFTD)'].columns\n",
    "# group_col = 'TN_group'\n",
    "# group_col = 'TN_group_1'\n",
    "# group_col = 'TN_group_15'\n",
    "group_col = 'linear_group'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34719108",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TN groupwise proportion of NACC copathology positivities\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example:\n",
    "# nacc_results: your dataframe\n",
    "# group_col: column with group info\n",
    "# status_cols: list of binary columns (0/1)\n",
    "\n",
    "status_cols = ['NACC_AD', 'NACC_PD', 'NACC_VD', 'NACC_LBD', 'NACC_PCA']  # example\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare data: proportion of positives and negatives\n",
    "# -----------------------------\n",
    "prop_nacc_results_list = []\n",
    "\n",
    "for col in status_cols:\n",
    "    # Compute proportion of positives per group\n",
    "    pos = nacc_results.groupby(group_col)[col].mean()\n",
    "    neg = 1 - pos  # proportion of negatives\n",
    "    \n",
    "    temp_nacc_results = pd.DataFrame({\n",
    "        'Group': pos.index,\n",
    "        'Positive': pos.values,\n",
    "        'Negative': neg.values,\n",
    "        'Condition': col\n",
    "    })\n",
    "    prop_nacc_results_list.append(temp_nacc_results)\n",
    "\n",
    "# Combine all conditions for plotting\n",
    "plot_nacc_results = pd.concat(prop_nacc_results_list)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot stacked barplot\n",
    "# -----------------------------\n",
    "conditions = plot_nacc_results['Condition'].unique()\n",
    "n_cols = 2\n",
    "n_rows = (len(conditions) + 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = {'Positive':'blue', 'Negative':'red'}\n",
    "\n",
    "for ax, cond in zip(axes, conditions):\n",
    "    nacc_results_cond = plot_nacc_results[plot_nacc_results['Condition'] == cond].set_index('Group')\n",
    "    nacc_results_cond[['Negative','Positive']].plot(\n",
    "        kind='bar',\n",
    "        stacked=True,\n",
    "        ax=ax,\n",
    "        color=[colors['Negative'], colors['Positive']],\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_ylabel('Proportion')\n",
    "    ax.set_title(cond)\n",
    "    ax.set_ylim(0,1)\n",
    "\n",
    "# Single legend for figure\n",
    "handles = [plt.Rectangle((0,0),1,1,color=colors[c]) for c in colors]\n",
    "fig.legend(handles, colors.keys(), loc='upper right', title='Status')\n",
    "\n",
    "# Remove unused axes\n",
    "for ax in axes[len(conditions):]:\n",
    "    ax.remove()\n",
    "plt.suptitle('NACC Pathology Positivity Proportions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute group-wise mean probabilities\n",
    "# ------------------------------------------------------------\n",
    "group_means = (\n",
    "    nacc_results\n",
    "    .groupby(group_col)[prob_cols]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot heatmap\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    group_means,\n",
    "    cmap=\"Reds\",\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    "    vmin=0,\n",
    "    vmax=0.4,\n",
    "    cbar_kws={\"label\": \"Mean predicted probability\"}\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Predicted pathology\")\n",
    "plt.ylabel(\"Subgroup\")\n",
    "plt.title(\"NACC Group-wise Mean Predicted Probability Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# prob_cols = nacc_results.loc[:, 'P(AD)':'P(svPPA)'].columns\n",
    "# prob_cols = nacc_results.loc[:, 'P(AD)':'P(bvFTD)'].columns\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sort: group first, then descending P(AD)\n",
    "# ------------------------------------------------------------\n",
    "nacc_sorted = (\n",
    "    nacc_results\n",
    "    .sort_values([group_col, \"P(AD)\"], ascending=[True, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "heatmap_data = nacc_sorted[prob_cols]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute group positions for y-axis labels\n",
    "# ------------------------------------------------------------\n",
    "group_counts = nacc_sorted[group_col].value_counts(sort=False)\n",
    "\n",
    "group_centers = {}\n",
    "start = 0\n",
    "\n",
    "for grp, count in group_counts.items():\n",
    "    center = start + count / 2\n",
    "    group_centers[grp] = center\n",
    "    start += count\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap=\"Reds\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    yticklabels=False,\n",
    "    cbar_kws={\"label\": \"Predicted probability\"}\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Horizontal lines between groups\n",
    "# ------------------------------------------------------------\n",
    "cum_sizes = np.cumsum(group_counts.values)\n",
    "\n",
    "for y in cum_sizes[:-1]:\n",
    "    ax.hlines(y, *ax.get_xlim(), colors=\"black\", linewidth=1.5)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TN subgroup labels on y-axis\n",
    "# ------------------------------------------------------------\n",
    "ax.set_yticks(list(group_centers.values()))\n",
    "ax.set_yticklabels(list(group_centers.keys()), rotation=0, fontsize=11)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Labels\n",
    "# ------------------------------------------------------------\n",
    "ax.set_xlabel(\"Predicted pathology\")\n",
    "ax.set_ylabel(\"TN subgroup\")\n",
    "ax.set_title(\"Subject-level Predicted Probability Heatmap\\n(sorted by descending P(AD))\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456dea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topic_cols = [c for c in nacc_results.columns if c.startswith(\"Topic_\")]\n",
    "\n",
    "groups = nacc_results[group_col].unique()\n",
    "n_groups = len(groups)\n",
    "n_topics = len(topic_cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global max for shared axis\n",
    "# ------------------------------------------------------------\n",
    "global_max = (\n",
    "    nacc_results\n",
    "    .groupby(group_col)[topic_cols]\n",
    "    .mean()\n",
    "    .values\n",
    "    .max()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Radar setup\n",
    "# ------------------------------------------------------------\n",
    "angles = np.linspace(0, 2 * np.pi, n_topics, endpoint=False)\n",
    "angles = np.concatenate([angles, [angles[0]]])\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, n_groups,\n",
    "    figsize=(4 * n_groups, 4),\n",
    "    subplot_kw=dict(polar=True)\n",
    ")\n",
    "\n",
    "if n_groups == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot\n",
    "# ------------------------------------------------------------\n",
    "for ax, grp in zip(axes, groups):\n",
    "\n",
    "    grp_df = nacc_results[nacc_results[group_col] == grp]\n",
    "    mean_topics = grp_df[topic_cols].mean().values\n",
    "    mean_topics = np.concatenate([mean_topics, [mean_topics[0]]])\n",
    "\n",
    "    ax.plot(angles, mean_topics, linewidth=2)\n",
    "    ax.fill(angles, mean_topics, alpha=0.25)\n",
    "\n",
    "    ax.set_title(grp, pad=20)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(topic_cols, fontsize=9)\n",
    "\n",
    "    ax.set_ylim(0, global_max * 1.1)   # âœ… shared scale\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "plt.suptitle(\"TN Subgroup Topic Weight Profiles (shared radial scale)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb85689",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NACC vulnerable check\n",
    "print('VULN')\n",
    "r2 = nacc_results[nacc_results[group_col]=='Vulnerable']\n",
    "print(r2['DX'].value_counts())\n",
    "print('\\nRESIL')\n",
    "r2 = nacc_results[nacc_results[group_col]=='Resilient']\n",
    "print(r2['DX'].value_counts())\n",
    "print('\\nCANON')\n",
    "r2 = nacc_results[nacc_results[group_col]=='Canonical']\n",
    "print(r2['DX'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2956c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation Subplots ##\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr  # or spearmanr if you prefer\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Example inputs\n",
    "# -----------------------------\n",
    "# nacc_results: your dataframe\n",
    "# cols_to_corr: list of columns of probabilities to correlate\n",
    "# target_col: column to correlate against\n",
    "cols_to_corr = prob_cols\n",
    "target_col = 'standardized_residual'  # for example\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting setup\n",
    "# -----------------------------\n",
    "n_cols = 3  # how many subplots per row\n",
    "n_rows = int(np.ceil(len(cols_to_corr) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "unique_groups = nacc_results[group_col].unique()\n",
    "palette = sns.color_palette(\"tab10\", n_colors=len(unique_groups))\n",
    "group_palette = dict(zip(unique_groups, palette))\n",
    "\n",
    "for ax, col in zip(axes, cols_to_corr):\n",
    "    \n",
    "    x = nacc_results[col]\n",
    "    y = nacc_results[target_col]\n",
    "    \n",
    "    # Compute correlation\n",
    "    r, p = pearsonr(x, y)\n",
    "    \n",
    "    # Scatter plot\n",
    "    sns.scatterplot(\n",
    "        x=x, y=y, hue=nacc_results[group_col], palette=group_palette, ax=ax, s=60, alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Fit line\n",
    "    sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', ci=None)\n",
    "    \n",
    "    # Annotate r and p\n",
    "    ax.text(0.05, 0.95, f\"r={r:.2f}\\np={p:.3f}\",\n",
    "            transform=ax.transAxes,\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(target_col)\n",
    "    ax.set_title(f\"{col} vs {target_col}\")\n",
    "\n",
    "# Remove empty axes if any\n",
    "for ax in axes[len(cols_to_corr):]:\n",
    "    ax.remove()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', title=group_col, bbox_to_anchor=(1.05, 1))\n",
    "plt.suptitle('NACC')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5ca10",
   "metadata": {},
   "source": [
    "**ADNI4 INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADNI4 Inference ## 260120\n",
    "adni4_raw = pd.read_csv('C:/Users/BREIN/Desktop/copathology_visualization_temp/data/stage_data/ptau_volume_model/ADNI4_3.csv')\n",
    "region_cols = adni4_raw.loc[:, 'VA/2':'VA/2035'].columns\n",
    "adni4_raw = adni4_raw.dropna(subset=region_cols)\n",
    "adni4_cn = adni4_raw[adni4_raw['DX'] == 'CN']\n",
    "adni4_pat = adni4_raw[adni4_raw['DX'] != 'CN']\n",
    "adni4_stage_df = adni4_pat[['FULL_ID', 'DX', 'tau_stage_aa/low', 'tau_stage_aa/mid', 'tau_stage_aa/high', 'pred_tau_stage_aa/low', 'pred_tau_stage_aa/mid', 'pred_tau_stage_aa/high']].dropna()\n",
    "prob_cols = ['pred_tau_stage_aa/low','pred_tau_stage_aa/mid','pred_tau_stage_aa/high']\n",
    "max_col = adni4_stage_df[prob_cols].idxmax(axis=1)\n",
    "adni4_stage_df[prob_cols] = 0\n",
    "adni4_stage_df.loc[:, prob_cols] = (pd.get_dummies(max_col).reindex(columns=prob_cols, fill_value=0).astype(float))\n",
    "stage_map = {\n",
    "    'low': 0,\n",
    "    'mid': 1,\n",
    "    'high': 2\n",
    "}\n",
    "def get_stage(colname):\n",
    "    return stage_map[colname.split('/')[-1]]\n",
    "adni4_stage_df['gt_stage'] = (\n",
    "    adni4_stage_df[['tau_stage_aa/low', 'tau_stage_aa/mid', 'tau_stage_aa/high']]\n",
    "    .idxmax(axis=1)\n",
    "    .apply(get_stage)\n",
    ")\n",
    "\n",
    "adni4_stage_df['pred_stage'] = (\n",
    "    adni4_stage_df[prob_cols]\n",
    "    .idxmax(axis=1)\n",
    "    .apply(get_stage)\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# Subject grouping\n",
    "# --------------------------------\n",
    "adni4_lower_than_pred = adni4_stage_df[\n",
    "    adni4_stage_df['gt_stage'] < adni4_stage_df['pred_stage']\n",
    "][['FULL_ID', 'DX', 'gt_stage', 'pred_stage']]\n",
    "\n",
    "adni4_exact_match = adni4_stage_df[\n",
    "    adni4_stage_df['gt_stage'] == adni4_stage_df['pred_stage']\n",
    "][['FULL_ID', 'DX', 'gt_stage', 'pred_stage']]\n",
    "\n",
    "adni4_prep = DataProcessor(\n",
    "    region_cols=region_cols,\n",
    "    dx_col='DX',\n",
    "    subject_col='FULL_ID'\n",
    ")\n",
    "adni4_prep.fit_baseline(hc_data=adni4_cn)\n",
    "adni4_Z = adni4_prep.compute_atrophy_scores(data=adni4_pat)\n",
    "print(adni4_Z.shape)\n",
    "print(adni4_cn.shape)\n",
    "\n",
    "adni4_theta = lda.transform(adni4_Z)\n",
    "adni4_y_pred = classifier.predict(adni4_theta)\n",
    "adni4_y_proba = classifier.predict_proba(adni4_theta)\n",
    "\n",
    "adni4_results = pd.DataFrame(adni4_theta, columns=[f\"Topic_{k}\" for k in range(lda.n_topics)])\n",
    "print(adni4_results.shape)\n",
    "\n",
    "subj_col = adni4_prep.subject_col\n",
    "if subj_col in adni4_pat.columns:\n",
    "    adni4_results.insert(0, \"SUBJ_ID\", adni4_pat[subj_col].values)\n",
    "\n",
    "adni4_results['pred_DX'] = adni4_y_pred\n",
    "for i, dx in enumerate(classifier.classes):\n",
    "    adni4_results[f\"P({dx})\"] = adni4_y_proba[:,i]\n",
    "\n",
    "adni4_results = adni4_results.merge(\n",
    "    adni4_raw[[\"FULL_ID\", \"DX\"]],\n",
    "    left_on=\"SUBJ_ID\",\n",
    "    right_on=\"FULL_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "adni4_results = adni4_results.drop(columns=[\"FULL_ID\"])\n",
    "\n",
    "adni4_results[\"SUBJ_ID\"] = adni4_results[\"SUBJ_ID\"].str.replace(\"_M\", \"_m\", regex=False)\n",
    "\n",
    "adni4_results = adni4_results.merge(\n",
    "    df_adni4_resilience[[\"SUBJ_ID\", \"TN_group\", \"standardized_residual\"]],\n",
    "    on=\"SUBJ_ID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "adni4_results = adni4_results.dropna(subset=['TN_group'])\n",
    "print('!!!!', adni4_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TN Group Margin Chage ##\n",
    "# Example column: 'standardized_residual'\n",
    "conditions = [\n",
    "    adni4_results['standardized_residual'] > 1.0,\n",
    "    adni4_results['standardized_residual'] < -1.0\n",
    "]\n",
    "\n",
    "choices = [\n",
    "    'Vulnerable',\n",
    "    'Resilient'\n",
    "]\n",
    "\n",
    "# Default is 'canonical'\n",
    "adni4_results['TN_group_1'] = np.select(conditions, choices, default='Canonical')\n",
    "\n",
    "# Quick check\n",
    "print(adni4_results[['standardized_residual', 'TN_group_1']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob_cols = nacc_results.loc[:,'P(AD)':'P(svPPA)'].columns\n",
    "prob_cols = ['P(AD)', 'P(MCI)', 'P(PD)', 'P(DLB)', 'P(SVAD)', 'P(bvFTD)', 'P(nfvPPA)', 'P(svPPA)']\n",
    "# nacc_results = nacc_results[nacc_results['DX']!='IMCI']######## TEMP\n",
    "# prob_cols = nacc_results.loc[:,'P(AD)':'P(bvFTD)'].columns\n",
    "group_col = 'TN_group'\n",
    "# group_col = 'TN_group_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f60e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Groupwise\n",
    "# ------------------------------------------------------------\n",
    "# Compute group-wise mean probabilities\n",
    "# ------------------------------------------------------------\n",
    "group_means = (\n",
    "    adni4_results\n",
    "    .groupby(group_col)[prob_cols]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot heatmap\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    group_means,\n",
    "    cmap=\"Reds\",\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    "    vmin=0,\n",
    "    vmax=0.4,\n",
    "    cbar_kws={\"label\": \"Mean predicted probability\"}\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Predicted pathology\")\n",
    "plt.ylabel(\"Subgroup\")\n",
    "plt.title(\"ADNI4 Group-wise Mean Predicted Probability Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bb4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subject-wise\n",
    "# ------------------------------------------------------------\n",
    "# Sort: group first, then descending P(AD)\n",
    "# ------------------------------------------------------------\n",
    "adni4_sorted = (\n",
    "    adni4_results\n",
    "    .sort_values([group_col, \"P(AD)\"], ascending=[True, False])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "heatmap_data = adni4_sorted[prob_cols]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute group positions for y-axis labels\n",
    "# ------------------------------------------------------------\n",
    "group_counts = adni4_sorted[group_col].value_counts(sort=False)\n",
    "\n",
    "group_centers = {}\n",
    "start = 0\n",
    "\n",
    "for grp, count in group_counts.items():\n",
    "    center = start + count / 2\n",
    "    group_centers[grp] = center\n",
    "    start += count\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    heatmap_data,\n",
    "    cmap=\"Reds\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    yticklabels=False,\n",
    "    cbar_kws={\"label\": \"Predicted probability\"}\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Horizontal lines between groups\n",
    "# ------------------------------------------------------------\n",
    "cum_sizes = np.cumsum(group_counts.values)\n",
    "\n",
    "for y in cum_sizes[:-1]:\n",
    "    ax.hlines(y, *ax.get_xlim(), colors=\"black\", linewidth=1.5)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TN subgroup labels on y-axis\n",
    "# ------------------------------------------------------------\n",
    "ax.set_yticks(list(group_centers.values()))\n",
    "ax.set_yticklabels(list(group_centers.keys()), rotation=0, fontsize=11)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Labels\n",
    "# ------------------------------------------------------------\n",
    "ax.set_xlabel(\"Predicted pathology\")\n",
    "ax.set_ylabel(\"TN subgroup\")\n",
    "ax.set_title(\"ADNI4 Subject-level Predicted Probability Heatmap\\n(sorted by descending P(AD))\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topic_cols = [c for c in adni4_results.columns if c.startswith(\"Topic_\")]\n",
    "\n",
    "groups = adni4_results[group_col].unique()\n",
    "n_groups = len(groups)\n",
    "n_topics = len(topic_cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global max for shared axis\n",
    "# ------------------------------------------------------------\n",
    "global_max = (\n",
    "    adni4_results\n",
    "    .groupby(group_col)[topic_cols]\n",
    "    .mean()\n",
    "    .values\n",
    "    .max()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Radar setup\n",
    "# ------------------------------------------------------------\n",
    "angles = np.linspace(0, 2 * np.pi, n_topics, endpoint=False)\n",
    "angles = np.concatenate([angles, [angles[0]]])\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, n_groups,\n",
    "    figsize=(4 * n_groups, 4),\n",
    "    subplot_kw=dict(polar=True)\n",
    ")\n",
    "\n",
    "if n_groups == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot\n",
    "# ------------------------------------------------------------\n",
    "for ax, grp in zip(axes, groups):\n",
    "\n",
    "    grp_df = adni4_results[adni4_results[group_col] == grp]\n",
    "    mean_topics = grp_df[topic_cols].mean().values\n",
    "    mean_topics = np.concatenate([mean_topics, [mean_topics[0]]])\n",
    "\n",
    "    ax.plot(angles, mean_topics, linewidth=2)\n",
    "    ax.fill(angles, mean_topics, alpha=0.25)\n",
    "\n",
    "    ax.set_title(grp, pad=20)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(topic_cols, fontsize=9)\n",
    "\n",
    "    ax.set_ylim(0, global_max * 1.1)   # âœ… shared scale\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "plt.suptitle(\"ADNI4 TN Subgroup Topic Weight Profiles (shared radial scale)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation Subplots ##\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr  # or spearmanr if you prefer\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Example inputs\n",
    "# -----------------------------\n",
    "# adni4_results: your dataframe\n",
    "# cols_to_corr: list of columns of probabilities to correlate\n",
    "# target_col: column to correlate against\n",
    "cols_to_corr = prob_cols\n",
    "target_col = 'standardized_residual'  # for example\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting setup\n",
    "# -----------------------------\n",
    "n_cols = 3  # how many subplots per row\n",
    "n_rows = int(np.ceil(len(cols_to_corr) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "unique_groups = adni4_results[group_col].unique()\n",
    "palette = sns.color_palette(\"tab10\", n_colors=len(unique_groups))\n",
    "group_palette = dict(zip(unique_groups, palette))\n",
    "\n",
    "for ax, col in zip(axes, cols_to_corr):\n",
    "    \n",
    "    x = adni4_results[col]\n",
    "    y = adni4_results[target_col]\n",
    "    \n",
    "    # Compute correlation\n",
    "    r, p = pearsonr(x, y)\n",
    "    \n",
    "    # Scatter plot\n",
    "    sns.scatterplot(\n",
    "        x=x, y=y, hue=adni4_results[group_col], palette=group_palette, ax=ax, s=60, alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Fit line\n",
    "    sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', ci=None)\n",
    "    \n",
    "    # Annotate r and p\n",
    "    ax.text(0.05, 0.95, f\"r={r:.2f}\\np={p:.3f}\",\n",
    "            transform=ax.transAxes,\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(target_col)\n",
    "    ax.set_title(f\"{col} vs {target_col}\")\n",
    "\n",
    "# Remove empty axes if any\n",
    "for ax in axes[len(cols_to_corr):]:\n",
    "    ax.remove()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', title=group_col, bbox_to_anchor=(1.05, 1))\n",
    "plt.suptitle('ADNI4')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adni4_stage_df\n",
    "real_low = adni4_stage_df[adni4_stage_df['gt_stage']==0]\n",
    "print(real_low['pred_stage'].value_counts())\n",
    "real_low[\"FULL_ID\"] = real_low[\"FULL_ID\"].str.replace(\"_M\", \"_m\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = pd.merge(adni4_results, real_low, left_on='SUBJ_ID', right_on='FULL_ID')\n",
    "print(r2['pred_stage'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
