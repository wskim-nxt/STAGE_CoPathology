{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148a1163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX\n",
      "NC        166\n",
      "AD         72\n",
      "svPPA      59\n",
      "bvFTD      53\n",
      "nfvPPA     46\n",
      "DLB        25\n",
      "PD         24\n",
      "SVAD       24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_processor import *\n",
    "from lda_model import LDATopicModel\n",
    "from classifier import TopicClassifier\n",
    "from visualizer import *\n",
    "from brain_visualizer import *\n",
    "\n",
    "data_path = 'C:/Users/WooSikKim/Desktop/Research/projects/co_pathology/scripts/stage_copath/data'\n",
    "\n",
    "inp_df = pd.read_csv(os.path.join(data_path,'train_data/260128_wsev_smc_combined_cn_included.csv'))\n",
    "inp_df = inp_df[inp_df['DX']!='HC'] # EXCLUDE WSEV HC\n",
    "\n",
    "print(inp_df['DX'].value_counts())\n",
    "\n",
    "df_nacc_resilience = pd.read_csv(data_path + '/nacc/NACC_resilience_inference.csv')\n",
    "df_adni4_resilience = pd.read_csv(data_path + '/adni/ADNI4_resilience_inference.csv')\n",
    "# df_nacc_resilience = pd.read_csv('C:/Users/BREIN/Desktop/stage_copath/20260122_NACC_linear_group.csv')\n",
    "\n",
    "df_adni4_resilience = df_adni4_resilience.rename(columns={\"FULL_ID\": \"SUBJ_ID\"})\n",
    "df_nacc_resilience = df_nacc_resilience.rename(columns={\"subject_id\" : \"SUBJ_ID\"})\n",
    "nacc_raw = pd.read_csv(data_path + '/nacc/260120_NACC_VA_TAU_PATH_matched.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9239bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr  # or spearmanr if you prefer\n",
    "def resilient_subgroup_visualization(inp_df,prob_cols, group_col,group_order,cohort='NACC',scatter_col='standardized_residual'):\n",
    "    group_means = (\n",
    "        inp_df\n",
    "        .groupby(group_col)[prob_cols]\n",
    "        .mean()\n",
    "        .reindex(group_order)\n",
    "    )\n",
    "###################### GROUP MEAN LEVEL #####################\n",
    "    # ------------------------------------------------------------\n",
    "    # Plot heatmap\n",
    "    # ------------------------------------------------------------\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    sns.heatmap(\n",
    "        group_means,\n",
    "        cmap=\"Reds\",\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        linewidths=0.5,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cbar_kws={\"label\": \"Mean predicted probability\"}\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Predicted pathology\")\n",
    "    plt.ylabel(\"Subgroup\")\n",
    "    plt.title(f\"{cohort} Group-wise Mean Predicted Probability Distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "###################### SUBJECT LEVEL #####################\n",
    "    # ------------------------------------------------------------\n",
    "    # Sort: group first, then descending P(AD)\n",
    "    # ------------------------------------------------------------\n",
    "    inp_df[group_col] = pd.Categorical(\n",
    "        inp_df[group_col],\n",
    "        categories=group_order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    df_sorted = (\n",
    "        inp_df\n",
    "        .sort_values([group_col, \"P(AD)\"], ascending=[True, False])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    heatmap_data = df_sorted[prob_cols]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Compute group positions for y-axis labels\n",
    "    # ------------------------------------------------------------\n",
    "    group_counts = (\n",
    "        df_sorted[group_col]\n",
    "        .value_counts()\n",
    "        .reindex(group_order)\n",
    "    )\n",
    "\n",
    "\n",
    "    group_centers = {}\n",
    "    start = 0\n",
    "\n",
    "    for grp, count in group_counts.items():\n",
    "        center = start + count / 2\n",
    "        group_centers[grp] = center\n",
    "        start += count\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Plot\n",
    "    # ------------------------------------------------------------\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        heatmap_data,\n",
    "        cmap=\"Reds\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        yticklabels=False,\n",
    "        cbar_kws={\"label\": \"Predicted probability\"}\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Horizontal lines between groups\n",
    "    # ------------------------------------------------------------\n",
    "    cum_sizes = np.cumsum(group_counts.values)\n",
    "\n",
    "    for y in cum_sizes[:-1]:\n",
    "        ax.hlines(y, *ax.get_xlim(), colors=\"black\", linewidth=1.5)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # TN subgroup labels on y-axis\n",
    "    # ------------------------------------------------------------\n",
    "    ax.set_yticks(list(group_centers.values()))\n",
    "    ax.set_yticklabels(list(group_centers.keys()), rotation=0, fontsize=11)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Labels\n",
    "    # ------------------------------------------------------------\n",
    "    ax.set_xlabel(\"Predicted pathology\")\n",
    "    ax.set_ylabel(\"Subgroup\")\n",
    "    ax.set_title(f\"{cohort} Subject-level Predicted Probability Heatmap\\n(sorted by descending P(AD))\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "###################### RADAR PLOT #####################\n",
    "    topic_cols = [c for c in inp_df.columns if c.startswith(\"Topic_\")]\n",
    "\n",
    "    groups = inp_df[group_col].unique()\n",
    "    n_groups = len(groups)\n",
    "    n_topics = len(topic_cols)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Global max for shared axis\n",
    "    # ------------------------------------------------------------\n",
    "    global_max = (\n",
    "        inp_df\n",
    "        .groupby(group_col)[topic_cols]\n",
    "        .mean()\n",
    "        .values\n",
    "        .max()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Radar setup\n",
    "    # ------------------------------------------------------------\n",
    "    angles = np.linspace(0, 2 * np.pi, n_topics, endpoint=False)\n",
    "    angles = np.concatenate([angles, [angles[0]]])\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        1, n_groups,\n",
    "        figsize=(4 * n_groups, 4),\n",
    "        subplot_kw=dict(polar=True)\n",
    "    )\n",
    "\n",
    "    if n_groups == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Plot\n",
    "    # ------------------------------------------------------------\n",
    "    # for ax, grp in zip(axes, groups):\n",
    "    for ax, grp in zip(axes, group_order):\n",
    "        print(ax,grp)\n",
    "\n",
    "        grp_df = inp_df[inp_df[group_col] == grp]\n",
    "        mean_topics = grp_df[topic_cols].mean().values\n",
    "        mean_topics = np.concatenate([mean_topics, [mean_topics[0]]])\n",
    "\n",
    "        ax.plot(angles, mean_topics, linewidth=2)\n",
    "        ax.fill(angles, mean_topics, alpha=0.25)\n",
    "\n",
    "        ax.set_title(grp, pad=20)\n",
    "\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(topic_cols, fontsize=9)\n",
    "\n",
    "        ax.set_ylim(0, global_max * 1.1)   # âœ… shared scale\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    plt.suptitle(f\"{cohort} Resilience Subgroup Topic Weight Profiles (shared radial scale)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "###################### CORRELATION SCATTER #####################\n",
    "    # -----------------------------\n",
    "    # Example inputs\n",
    "    # -----------------------------\n",
    "    # inp_df: your dataframe\n",
    "    # cols_to_corr: list of columns of probabilities to correlate\n",
    "    # target_col: column to correlate against\n",
    "    cols_to_corr = prob_cols\n",
    "    target_col = scatter_col  # for example\n",
    "\n",
    "    # -----------------------------\n",
    "    # Plotting setup\n",
    "    # -----------------------------\n",
    "    n_cols = 3  # how many subplots per row\n",
    "    n_rows = int(np.ceil(len(cols_to_corr) / n_cols))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    palette = sns.color_palette(\"tab10\", n_colors=len(group_order))\n",
    "\n",
    "    group_palette = dict(zip(group_order, palette))\n",
    "\n",
    "    for ax, col in zip(axes, cols_to_corr):\n",
    "        \n",
    "        x = inp_df[col]\n",
    "        y = inp_df[target_col]\n",
    "        \n",
    "        # Compute correlation\n",
    "        r, p = pearsonr(x, y)\n",
    "        \n",
    "        # Scatter plot\n",
    "        sns.scatterplot(\n",
    "            x=x, y=y, hue=inp_df[group_col], palette=group_palette, ax=ax, s=60, alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # Fit line\n",
    "        sns.regplot(x=x, y=y, ax=ax, scatter=False, color='red', ci=None)\n",
    "        \n",
    "        # Annotate r and p\n",
    "        ax.text(0.05, 0.95, f\"r={r:.2f}\\np={p:.3f}\",\n",
    "                transform=ax.transAxes,\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7),\n",
    "                fontsize = 13)\n",
    "        \n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel(target_col)\n",
    "        ax.set_title(f\"{col} vs {target_col}\")\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([-3, 4])\n",
    "\n",
    "    # Remove empty axes if any\n",
    "    for ax in axes[len(cols_to_corr):]:\n",
    "        ax.remove()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    fig.legend(handles, labels, loc='upper right', title=group_col, bbox_to_anchor=(1.05, 1))\n",
    "    plt.suptitle('NACC')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5dd6ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX_new\n",
      "non-AD    148\n",
      "NC        100\n",
      "AD         72\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "region_cols = nacc_raw.loc[:, 'VA/2':'VA/2035'].columns\n",
    "# nacc_filtered = nacc_raw[nacc_raw['DX'] != 'Unknown']\n",
    "# nacc_cn = nacc_filtered[nacc_filtered['DX'] == 'CN']\n",
    "\n",
    "\n",
    "## DOWNSAMPLE LARGE DX \n",
    "N = 25\n",
    "dx_col = \"DX\"\n",
    "balanced_parts = []\n",
    "\n",
    "for dx, g in inp_df.groupby(dx_col):\n",
    "    if dx == 'AD':\n",
    "        N=100 ##\n",
    "    elif dx == 'NC':\n",
    "        N=100 ##\n",
    "    else: \n",
    "        N=25\n",
    "    if len(g) > N:\n",
    "        g = g.sample(n=N, replace=False, random_state=42)\n",
    "    balanced_parts.append(g)\n",
    "\n",
    "train_df = pd.concat(balanced_parts).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#### add mci to AD ####\n",
    "# train_df['DX'] = train_df['DX'].replace({'MCI' : 'AD'})\n",
    "\n",
    "#### collapse all non-ad to one DX ##\n",
    "train_df['DX_new'] = np.where(train_df['DX'].isin(['AD', 'NC']), train_df['DX'], 'non-AD')\n",
    "print(train_df['DX_new'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b29494",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = range(4,26,2)\n",
    "perplexities = []\n",
    "cv_acc = []\n",
    "dx_label = 'DX_new'\n",
    "for k in [8]:\n",
    "    print('K-topics = ', k)\n",
    "    labels = train_df[dx_label].values\n",
    "    ids = train_df[\"SUBJ_ID\"].values\n",
    "\n",
    "    lda = LDATopicModel(n_topics=k, alpha=1/k, beta=1/k)\n",
    "    # lda = LDATopicModel(n_topics=k)\n",
    "    print(lda.alpha, lda.beta)\n",
    "    theta = lda.fit_transform(train_df[region_cols])\n",
    "    classifier = TopicClassifier(n_splits=5)\n",
    "    cv_results = classifier.cross_validate(theta, labels, ids, verbose=False)\n",
    "    classifier.fit(theta, labels)\n",
    "    print(f\"k_topics {k}, CV ACC: {cv_results['accuracy']}\")\n",
    "    print(lda._theta.shape)\n",
    "    \n",
    "    perplexities.append(lda._perplexity)\n",
    "    cv_acc.append(cv_results['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_management",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
